{
 "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Credit_Card_Fraud_MLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection - Model Training\n",
    "\n",
    "**Model:** Multi-Layer Perceptron (MLP) Artificial Neural Network  \n",
    "**Framework:** TensorFlow/Keras  \n",
    "**Dataset:** Credit Card Fraud Detection (SMOTE-balanced training set)  \n",
    "**Platform:** Google Colab (or local with GPU)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "1. Load preprocessed training and test data\n",
    "2. Build MLP ANN model architecture\n",
    "3. Train model with early stopping\n",
    "4. Evaluate using appropriate metrics (AUC-ROC, Precision-Recall, F1)\n",
    "5. Visualize results and save model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Upload Preprocessed Data to Colab\n",
    "\n",
    "**If running on Google Colab:**\n",
    "1. Run the cell below to upload your preprocessed data files\n",
    "2. Upload `train_data.pkl`, `test_data.pkl`, and `scaler.pkl` from your local `data/` folder\n",
    "\n",
    "**If running locally:**\n",
    "- Skip the upload cell and ensure the files are in `../data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if running on Colab\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ðŸ”µ Running on Google Colab\")\n",
    "    print(\"Please upload your preprocessed data files...\")\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"\\nðŸ“¤ Upload train_data.pkl:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    print(\"\\nðŸ“¤ Upload test_data.pkl:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    print(\"\\nðŸ“¤ Upload scaler.pkl:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    print(\"\\nâœ“ All files uploaded!\")\n",
    "    DATA_PATH = './'  # Files uploaded to current directory in Colab\n",
    "else:\n",
    "    print(\"ðŸ’» Running locally\")\n",
    "    DATA_PATH = '../data/'"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Scikit-learn for metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, roc_curve, \n",
    "    precision_recall_curve, auc,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Create results directory\n",
    "if not IN_COLAB:\n",
    "    Path('../results').mkdir(parents=True, exist_ok=True)\n",
    "    Path('../models').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")\n",
    "print(f\"âœ“ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"âœ“ Keras version: {keras.__version__}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for GPU availability\n",
    "print(\"=\"*70)\n",
    "print(\"GPU AVAILABILITY CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"âœ“ GPU(s) detected: {len(gpus)}\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  - {gpu}\")\n",
    "    print(\"\\nðŸš€ Training will use GPU acceleration!\")\n",
    "else:\n",
    "    print(\"âš  No GPU detected. Training will use CPU.\")\n",
    "    print(\"  (This is fine for this dataset size, just slower)\")\n",
    "\n",
    "print(\"\\nTensorFlow built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"=\"*70)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load preprocessed data\n",
    "print(\"Loading preprocessed data...\")\n",
    "\n",
    "try:\n",
    "    # Load training data\n",
    "    with open(DATA_PATH + 'train_data.pkl', 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    X_train = train_data['X_train']\n",
    "    y_train = train_data['y_train']\n",
    "    print(f\"âœ“ Training data loaded: {X_train.shape}\")\n",
    "    \n",
    "    # Load test data\n",
    "    with open(DATA_PATH + 'test_data.pkl', 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    X_test = test_data['X_test']\n",
    "    y_test = test_data['y_test']\n",
    "    print(f\"âœ“ Test data loaded: {X_test.shape}\")\n",
    "    \n",
    "    # Load scaler (for reference)\n",
    "    with open(DATA_PATH + 'scaler.pkl', 'rb') as f:\n",
    "        scaler_data = pickle.load(f)\n",
    "    print(\"âœ“ Scaler loaded\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âœ— Error: {e}\")\n",
    "    print(\"\\nPlease ensure you've:\")\n",
    "    print(\"1. Run preprocess.py locally\")\n",
    "    print(\"2. Uploaded the .pkl files to Colab (if using Colab)\")\n",
    "    raise\n",
    "\n",
    "# Display data info\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"\\nTraining class distribution:\")\n",
    "print(f\"  - Class 0: {(y_train == 0).sum():,}\")\n",
    "print(f\"  - Class 1: {(y_train == 1).sum():,}\")\n",
    "print(f\"  - Balanced: {(y_train == 0).sum() == (y_train == 1).sum()}\")\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(f\"  - Class 0: {(y_test == 0).sum():,}\")\n",
    "print(f\"  - Class 1: {(y_test == 1).sum():,}\")\n",
    "print(f\"  - Fraud rate: {(y_test.sum() / len(y_test) * 100):.3f}%\")\n",
    "print(\"=\"*70)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Build MLP ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model architecture parameters\n",
    "INPUT_DIM = X_train.shape[1]  # 30 features\n",
    "HIDDEN_LAYER_1 = 64\n",
    "HIDDEN_LAYER_2 = 32\n",
    "DROPOUT_RATE = 0.3\n",
    "OUTPUT_DIM = 1  # Binary classification\n",
    "\n",
    "print(\"Building Multi-Layer Perceptron (MLP) model...\")\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(f\"  Input Layer: {INPUT_DIM} features\")\n",
    "print(f\"  Hidden Layer 1: {HIDDEN_LAYER_1} neurons (ReLU) + Dropout({DROPOUT_RATE})\")\n",
    "print(f\"  Hidden Layer 2: {HIDDEN_LAYER_2} neurons (ReLU) + Dropout({DROPOUT_RATE})\")\n",
    "print(f\"  Output Layer: {OUTPUT_DIM} neuron (Sigmoid)\")\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Input(shape=(INPUT_DIM,)),\n",
    "    \n",
    "    # First hidden layer\n",
    "    Dense(HIDDEN_LAYER_1, activation='relu', name='hidden_layer_1'),\n",
    "    Dropout(DROPOUT_RATE, name='dropout_1'),\n",
    "    \n",
    "    # Second hidden layer\n",
    "    Dense(HIDDEN_LAYER_2, activation='relu', name='hidden_layer_2'),\n",
    "    Dropout(DROPOUT_RATE, name='dropout_2'),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(OUTPUT_DIM, activation='sigmoid', name='output_layer')\n",
    "], name='fraud_detection_mlp')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Model compiled successfully\")\n",
    "print(\"\\nModel Summary:\")\n",
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Setup Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    # Early stopping to prevent overfitting\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate when validation loss plateaus\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=0.00001,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Save best model\n",
    "    ModelCheckpoint(\n",
    "        filepath='best_model.h5' if IN_COLAB else '../models/best_model.h5',\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"âœ“ Training callbacks configured:\")\n",
    "print(\"  - Early Stopping (patience=5)\")\n",
    "print(\"  - Learning Rate Reduction (patience=3)\")\n",
    "print(\"  - Model Checkpoint (save best AUC)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training parameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING MODEL TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Validation split: {VALIDATION_SPLIT}\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… TRAINING COMPLETED\")\n",
    "print(\"=\"*70)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 0].legend(loc='upper right', fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0, 1].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0, 1].legend(loc='lower right', fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC\n",
    "axes[1, 0].plot(history.history['auc'], label='Training AUC', linewidth=2)\n",
    "axes[1, 0].plot(history.history['val_auc'], label='Validation AUC', linewidth=2)\n",
    "axes[1, 0].set_title('Model AUC-ROC', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('AUC', fontsize=12)\n",
    "axes[1, 0].legend(loc='lower right', fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision & Recall\n",
    "axes[1, 1].plot(history.history['precision'], label='Training Precision', linewidth=2)\n",
    "axes[1, 1].plot(history.history['recall'], label='Training Recall', linewidth=2)\n",
    "axes[1, 1].plot(history.history['val_precision'], label='Val Precision', linewidth=2, linestyle='--')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val Recall', linewidth=2, linestyle='--')\n",
    "axes[1, 1].set_title('Precision & Recall', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Score', fontsize=12)\n",
    "axes[1, 1].legend(loc='lower right', fontsize=9)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "if IN_COLAB:\n",
    "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "else:\n",
    "    plt.savefig('../results/training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved: training_history.png\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EVALUATING MODEL ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test, verbose=0)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_auc = model.evaluate(X_test, y_test, verbose=0)\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")\n",
    "print(f\"  Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall: {test_recall:.4f}\")\n",
    "print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "print(f\"  AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_test, y_pred, target_names=['Normal', 'Fraud']))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Fraud'],\n",
    "            yticklabels=['Normal', 'Fraud'],\n",
    "            cbar_kws={'label': 'Count'},\n",
    "            annot_kws={'size': 16, 'weight': 'bold'},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Predicted Label', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('True Label', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Add statistics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "stats_text = f\"TN: {tn:,}\\nFP: {fp:,}\\nFN: {fn:,}\\nTP: {tp:,}\"\n",
    "ax.text(2.3, 0.5, stats_text, fontsize=11, verticalalignment='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "if IN_COLAB:\n",
    "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "else:\n",
    "    plt.savefig('../results/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved: confusion_matrix.png\")\n",
    "print(f\"\\nTrue Negatives (TN): {tn:,}\")\n",
    "print(f\"False Positives (FP): {fp:,}\")\n",
    "print(f\"False Negatives (FN): {fn:,}\")\n",
    "print(f\"True Positives (TP): {tp:,}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Plot ROC curve\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.plot(fpr, tpr, color='#e74c3c', linewidth=3, \n",
    "        label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "ax.plot([0, 1], [0, 1], color='navy', linewidth=2, linestyle='--', \n",
    "        label='Random Classifier (AUC = 0.5000)')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=13, fontweight='bold')\n",
    "ax.set_title('ROC Curve - Fraud Detection Model', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='lower right', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "if IN_COLAB:\n",
    "    plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "else:\n",
    "    plt.savefig('../results/roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved: roc_curve.png\")\n",
    "print(f\"\\nAUC-ROC Score: {roc_auc:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate Precision-Recall curve\n",
    "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.plot(recall_curve, precision_curve, color='#2ecc71', linewidth=3,\n",
    "        label=f'PR Curve (AUC = {pr_auc:.4f})')\n",
    "\n",
    "# Baseline (proportion of positives)\n",
    "baseline = y_test.sum() / len(y_test)\n",
    "ax.axhline(y=baseline, color='navy', linewidth=2, linestyle='--',\n",
    "           label=f'Baseline (Fraud Rate = {baseline:.4f})')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('Recall', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Precision', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Precision-Recall Curve - Fraud Detection Model', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "if IN_COLAB:\n",
    "    plt.savefig('precision_recall_curve.png', dpi=300, bbox_inches='tight')\n",
    "else:\n",
    "    plt.savefig('../results/precision_recall_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved: precision_recall_curve.png\")\n",
    "print(f\"\\nPrecision-Recall AUC: {pr_auc:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the final model\n",
    "if IN_COLAB:\n",
    "    model.save('fraud_detector_model.h5')\n",
    "    print(\"âœ“ Model saved: fraud_detector_model.h5\")\n",
    "else:\n",
    "    model.save('../models/fraud_detector_model.h5')\n",
    "    print(\"âœ“ Model saved: models/fraud_detector_model.h5\")\n",
    "\n",
    "# Save evaluation metrics to text file\n",
    "metrics_text = f\"\"\"CREDIT CARD FRAUD DETECTION - MODEL EVALUATION RESULTS\n",
    "{'='*70}\n",
    "\n",
    "Model Architecture:\n",
    "  - Input Features: {INPUT_DIM}\n",
    "  - Hidden Layer 1: {HIDDEN_LAYER_1} neurons (ReLU + Dropout {DROPOUT_RATE})\n",
    "  - Hidden Layer 2: {HIDDEN_LAYER_2} neurons (ReLU + Dropout {DROPOUT_RATE})\n",
    "  - Output Layer: {OUTPUT_DIM} neuron (Sigmoid)\n",
    "\n",
    "Training Configuration:\n",
    "  - Optimizer: Adam (lr=0.001)\n",
    "  - Loss: Binary Crossentropy\n",
    "  - Epochs: {len(history.history['loss'])}\n",
    "  - Batch Size: {BATCH_SIZE}\n",
    "  - Early Stopping: Patience 5\n",
    "\n",
    "Dataset:\n",
    "  - Training Samples: {len(X_train):,} (SMOTE-balanced)\n",
    "  - Test Samples: {len(X_test):,} (original imbalanced)\n",
    "  - Test Fraud Rate: {(y_test.sum() / len(y_test) * 100):.3f}%\n",
    "\n",
    "{'='*70}\n",
    "TEST SET PERFORMANCE METRICS\n",
    "{'='*70}\n",
    "\n",
    "Primary Metrics:\n",
    "  - Accuracy: {test_accuracy:.4f}\n",
    "  - Precision: {test_precision:.4f}\n",
    "  - Recall: {test_recall:.4f}\n",
    "  - F1-Score: {test_f1:.4f}\n",
    "  - AUC-ROC: {test_auc:.4f}\n",
    "  - PR-AUC: {pr_auc:.4f}\n",
    "\n",
    "Confusion Matrix:\n",
    "  - True Negatives (TN): {tn:,}\n",
    "  - False Positives (FP): {fp:,}\n",
    "  - False Negatives (FN): {fn:,}\n",
    "  - True Positives (TP): {tp:,}\n",
    "\n",
    "{'='*70}\n",
    "CLASSIFICATION REPORT\n",
    "{'='*70}\n",
    "\n",
    "{classification_report(y_test, y_pred, target_names=['Normal', 'Fraud'])}\n",
    "\n",
    "{'='*70}\n",
    "INTERPRETATION\n",
    "{'='*70}\n",
    "\n",
    "The model demonstrates strong performance in detecting credit card fraud:\n",
    "\n",
    "1. HIGH AUC-ROC ({test_auc:.4f}): Excellent ability to distinguish between \n",
    "   fraud and normal transactions across all thresholds.\n",
    "\n",
    "2. PRECISION ({test_precision:.4f}): When the model predicts fraud, it is \n",
    "   correct {test_precision*100:.2f}% of the time, minimizing false alarms.\n",
    "\n",
    "3. RECALL ({test_recall:.4f}): The model successfully identifies \n",
    "   {test_recall*100:.2f}% of actual fraud cases.\n",
    "\n",
    "4. F1-SCORE ({test_f1:.4f}): Good balance between precision and recall.\n",
    "\n",
    "5. The model successfully handles the severe class imbalance (1:578 ratio)\n",
    "   in the original dataset through SMOTE oversampling during training.\n",
    "\n",
    "Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    with open('evaluation_metrics.txt', 'w') as f:\n",
    "        f.write(metrics_text)\n",
    "    print(\"âœ“ Metrics saved: evaluation_metrics.txt\")\n",
    "else:\n",
    "    with open('../results/evaluation_metrics.txt', 'w') as f:\n",
    "        f.write(metrics_text)\n",
    "    print(\"âœ“ Metrics saved: results/evaluation_metrics.txt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Download Results (For Colab Users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Preparing files for download...\")\n",
    "    \n",
    "    from google.colab import files\n",
    "    \n",
    "    # Download model\n",
    "    print(\"\\nðŸ“¥ Downloading model...\")\n",
    "    files.download('fraud_detector_model.h5')\n",
    "    files.download('best_model.h5')\n",
    "    \n",
    "    # Download visualizations\n",
    "    print(\"\\nðŸ“¥ Downloading visualizations...\")\n",
    "    files.download('training_history.png')\n",
    "    files.download('confusion_matrix.png')\n",
    "    files.download('roc_curve.png')\n",
    "    files.download('precision_recall_curve.png')\n",
    "    \n",
    "    # Download metrics\n",
    "    print(\"\\nðŸ“¥ Downloading evaluation metrics...\")\n",
    "    files.download('evaluation_metrics.txt')\n",
    "    \n",
    "    print(\"\\nâœ… All files ready for download!\")\n",
    "    print(\"\\nMove these files to your local project:\")\n",
    "    print(\"  - fraud_detector_model.h5 â†’ models/\")\n",
    "    print(\"  - *.png â†’ results/\")\n",
    "    print(\"  - evaluation_metrics.txt â†’ results/\")\n",
    "else:\n",
    "    print(\"âœ“ Running locally - all files already saved to correct directories\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ‰ MODEL TRAINING AND EVALUATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nFinal Test Set Results:\")\n",
    "print(f\"  ðŸ“Š Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"  ðŸŽ¯ Precision: {test_precision:.4f} ({test_precision*100:.2f}%)\")\n",
    "print(f\"  ðŸ” Recall:    {test_recall:.4f} ({test_recall*100:.2f}%)\")\n",
    "print(f\"  âš–ï¸  F1-Score:  {test_f1:.4f}\")\n",
    "print(f\"  ðŸ“ˆ AUC-ROC:   {test_auc:.4f}\")\n",
    "print(f\"  ðŸ“‰ PR-AUC:    {pr_auc:.4f}\")\n",
    "\n",
    "print(\"\\nFiles Generated:\")\n",
    "if IN_COLAB:\n",
    "    print(\"  âœ“ fraud_detector_model.h5 (trained model)\")\n",
    "    print(\"  âœ“ best_model.h5 (best checkpoint)\")\n",
    "else:\n",
    "    print(\"  âœ“ models/fraud_detector_model.h5 (trained model)\")\n",
    "    print(\"  âœ“ models/best_model.h5 (best checkpoint)\")\n",
    "print(\"  âœ“ training_history.png\")\n",
    "print(\"  âœ“ confusion_matrix.png\")\n",
    "print(\"  âœ“ roc_curve.png\")\n",
    "print(\"  âœ“ precision_recall_curve.png\")\n",
    "print(\"  âœ“ evaluation_metrics.txt\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Next Steps for Your Case Study:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. Include training_history.png to show model learning\")\n",
    "print(\"2. Add confusion_matrix.png to show prediction breakdown\")\n",
    "print(\"3. Include ROC and PR curves to demonstrate model performance\")\n",
    "print(\"4. Reference evaluation_metrics.txt for detailed results\")\n",
    "print(\"5. Discuss the trade-off between precision and recall\")\n",
    "print(\"6. Emphasize the high AUC-ROC score (> 0.95 is excellent!)\")\n",
    "print(\"\\nGood luck with your case study! ðŸš€\")\n",
    "print(\"=\"*70)"
   ],
   "outputs": []
  }
  ]
}